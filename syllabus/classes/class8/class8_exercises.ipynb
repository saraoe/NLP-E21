{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises on attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product\n",
    "1) Calculate the dot product between two word embedding which you believe are similar\n",
    "2) Calculate the dot product between the two word and a word which you believe is dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.906174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model['man'], model['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3584146"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model['house'], model['jump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.504671"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model['man'], model['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.239387"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model['man'], model['house'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.118877"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(model['woman'], model['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) make the three words into a matrix $E$ and multiply it with its own transpose using matrix multiplication. So $E \\cdot E^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27.504673, 25.906172, 15.239388],\n",
       "       [25.906172, 31.081322, 15.118874],\n",
       "       [15.239388, 15.118874, 26.629263]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.array([model['man'], model['woman'], model['house']])\n",
    "np.matmul(E, E.T)\n",
    "# This is equal to the dot product between the word embeddings \n",
    "# - e.g. first col = dot(man, man), dot(man, woman), dot(man, house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "QK =np.matmul(E, E.T)\n",
    "plt.imshow(QK, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Examine the attention formula from Vaswani et al. (2017), you have now implemented $Q\\cdot K^T$\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d}}) \\cdot V\n",
    "$$\n",
    "Where $d$ is the dimension of of the embedding and Q, K, V stands for queries, keys and values.\n",
    "\n",
    "\n",
    "  - 4.1) Now add the softmax. Examining the outcome, how come that the matrix is no longer symmetric?\n",
    "  - 4.2) Now normalize the using the $\\sqrt{d}$, how does this change the outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array([model['man'], model['woman'], model['house']])\n",
    "QK = np.matmul(E, E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3181e-01, 5.6236e-03, 1.1309e-05],\n",
       "        [1.6819e-01, 9.9438e-01, 1.0025e-05],\n",
       "        [3.9199e-06, 1.1618e-07, 9.9998e-01]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_tensor = torch.tensor(QK)\n",
    "soft = torch.nn.Softmax(dim=0)\n",
    "soft(QK_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.889748 , 3.663686 , 2.155175 ],\n",
       "       [3.663686 , 4.3955626, 2.1381316],\n",
       "       [2.155175 , 2.1381316, 3.7659464]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 50\n",
    "QK/np.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = soft(torch.tensor(QK/np.sqrt(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAADnCAYAAADByJnJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM4UlEQVR4nO3dX4hm913H8fdnJqyCVihMRdld0kU31KWVVrebCyEtJZEJ4i5YlU0VGlBToWtLg+IGZZFVEKvUC7tIgwS8SdfYCxlxdAVtEcWWGUxQduPGYRV39qbdmuqFSbZTv17Ms/HJdHae82SeM885x/crHJhznt9+z+8mH36/3/mXqkKS2rQw7w5IGj6DRlLrDBpJrTNoJLXOoJHUuvv2+vHkb3zeS1J7+Njy9867C5330NvfNu8udN7bl741+/n3i99xf9XWK43a1itfuVJVy/s535uxZ9BI6r7aepVvecfZRm1fff73llruzq4MGqnvAmRfg6LWuUYjDUEWmm1NSiXLSa4n2UhyfpfffzfJC6PtpSRfm1TTEY00BDMa0SRZBC4BjwCbwFqSlaq6drdNVX1irP0vAO+ZVNcRjdR7gYXFZttkp4CNqrpRVXeAy8CZPdo/Bnx2UlFHNFLfhcbTImApyfrY/tNV9fTY/mHg5tj+JvDgrqdN7geOAX896aQGjdR7mWbqdLuqTs7oxGeBz1XVNyY1NGikIWg+opnkFnB0bP/I6NhuzgIfbVLUNRppCJJm22RrwPEkx5IcYjtMVr75dHkH8Fbg75sUNWik3svMLm9X1RZwDrgCvAg8V1VXk1xMcnqs6VngcjV8oZVTJ6nvQtMrSo1U1SqwuuPYhR37vzZNTYNG6r3Mco2mFQaNNAQL3X4EwaCR+m66+2jmwqCRhqDjD1UaNFLvZaaLwW0waKQhcOokqVXNb8abG4NGGgJHNJJa54hGUru8YU9S22b8CEIbDBqp9xzRSDoIrtFIap0jGkmtc0QjqVVxjUbSAciCQSOpRdtfxHXqJKlNGW0dZtBIvRdHNJLaZ9BIat2Ci8GSWuUajaS2pQdrNN0eb0lqJEmjrWGt5STXk2wkOX+PNj+Z5FqSq0menVTTEY00ALMa0SRZBC4BjwCbwFqSlaq6NtbmOPAU8ENV9XKS75xU1xGNNAAzHNGcAjaq6kZV3QEuA2d2tPk54FJVvQxQVV+eVNSgkfoukIU02oClJOtj2xM7qh0Gbo7tb46OjXsAeCDJ3yX5YpLlSV106iT13JSLwber6uQ+T3kfcBx4P3AE+Jsk76qqr93rHziikQZghlOnW8DRsf0jo2PjNoGVqvp6Vf0r8BLbwXNPBo00BGm4TbYGHE9yLMkh4CywsqPNn7A9miHJEttTqRt7FXXqJPVdZnfVqaq2kpwDrgCLwDNVdTXJRWC9qlZGv/1wkmvAN4Bfqqqv7lXXoJEGYJY37FXVKrC649iFsb8LeHK0NWLQSD0X4rNOkg5At59AMGik3pvhGk1bDBppAAwaSa0zaCS1bvR4QWcZNFLPTfMKiHkxaKQBMGgktc6gkdS+bueMQSMNgSMaSa1KYMGrTpLa5VUnSQeg4zlj0EhD4IhGUrviiEZSy4KLwZIOgEEjqV1OnSS1LbgYLKl13kcj6QB0PGcMGqn3fARBUttco5F0IDqeM357WxqCu6/znLQ1rLWc5HqSjSTnd/n98SRfSfLCaPvZSTUd0UgDMKsRTZJF4BLwCLAJrCVZqaprO5r+UVWda1rXEY3Ud5npiOYUsFFVN6rqDnAZOLPfLu45onn3A2/bb/1B+8hv/uW8u9B5//z0T827C4O3/e3txkOapSTrY/tPV9XTY/uHgZtj+5vAg7vU+WCSh4CXgE9U1c1d2rzOqZM0AFNMnW5X1cl9nu5Pgc9W1WtJPgL8IfCBvf6BUydpAGY4dboFHB3bPzI69rqq+mpVvTba/QPgBycVNWikvhs9VNlka2ANOJ7kWJJDwFlg5Q2nS757bPc08OKkok6dpJ6b5Q17VbWV5BxwBVgEnqmqq0kuAutVtQJ8LMlpYAv4D+DxSXUNGmkAZnlncFWtAqs7jl0Y+/sp4Klpaho00gD4rJOkdvniK0lti++jkXQQOp4zBo00BAsdTxqDRuo5v70t6UB0PGcMGmkIXAyW1LqO54xBI/Vd2L7E3WUGjTQArtFIalemevHVXBg0Us8F76ORdAA6njMGjTQEXt6W1Kop3p43NwaNNACLHU8ag0YaAKdOklq1fdVp3r3Ym0Ej9d0U39WeF4NGGoCO54xBIw2BIxpJrQqw2PFFGoNGGoBux4yfxJV6L9l+1qnJ1qxelpNcT7KR5Pwe7T6YpJKcnFTToJEGYFbf3k6yCFwCHgVOAI8lObFLu7cAHwe+1KR/Bo00ABld4p60NXAK2KiqG1V1B7gMnNml3a8DvwW82qSoQSMNwBQjmqUk62PbEztKHQZuju1vjo6NnSs/ABytqj9r2j8Xg6WeSzLNVafbVTVxTWWPcy0AnwIen+bfGTTSAMzwPppbwNGx/SOjY3e9BXgn8IXROb8LWElyuqrW71XUoJEGYIZrIGvA8STH2A6Ys8CH7v5YVf8JLN3dT/IF4Bf3CpkZ90/SPITZLQZX1RZwDrgCvAg8V1VXk1xMcvrN9tERjTQAs7wxuKpWgdUdxy7co+37m9Q0aKSeS3wEQdIB6HjOGDTSEHT84W2DRuo7v+sk6UB0/fKxQSMNQMcHNAaN1HdTPoIwFwaNNAAdzxmDRuo7F4MlHYiO54xBI/VenDpJOgDp+OvJDRqp5wLc1/EbaQwaaQD8gJykVm1fdZp3L/Zm0Eh91/BTKvNk0EgD4H00klq1/e3tefdibwaN1Hthwcvbktq0/XLyefdibwaN1HfeGSzpILgYLKlVTp0kHYiuv/iq4xfFJE0Stv9HbrI1qpcsJ7meZCPJ+V1+//kk/5TkhSR/m+TEpJoGjdR3md0ncZMsApeAR4ETwGO7BMmzVfWuqno38EngU5PqGjTSAKTh1sApYKOqblTVHeAycGa8QVX919jutwE1qahrNFLPzfhVnoeBm2P7m8CD33TO5KPAk8Ah4AOTijqikQZgihHNUpL1se2JN3O+qrpUVd8D/DLwq5PaO6KRei8sNL/qdLuqTu7x+y3g6Nj+kdGxe7kM/P6kkzqikXpuxled1oDjSY4lOQScBVbecL7k+NjujwD/MqmoIxppAGb1hr2q2kpyDrgCLALPVNXVJBeB9apaAc4leRj4OvAy8OFJdQ0aaQBmebteVa0CqzuOXRj7++PT1twzaH7nR79v2nr/r3z6x9457y503lvfe27eXei8V57/9P4KxHcGS2pZgEWDRlLbuh0zBo00CB0f0Bg0Ut9tX97udtIYNNIAOKKR1LL47W1J7fKqk6T2+aVKSQfBoJHUOtdoJLVq+8VX8+7F3gwaaQD8rpOk1jl1ktQqp06SDoA37Elqm/fRSDoIHc8Zg0bqOx9BkHQwup0zBo00BC4GS2pdx2dOBo00BB3PGYNGGoSOJ41BI/Vc0v1nnfz2tjQAabg1qpUsJ7meZCPJ+V1+fzLJtST/mOSvktw/qaZBIw3BjJImySJwCXgUOAE8luTEjmbPAyer6vuBzwGfnFTXoJF6L43/a+AUsFFVN6rqDnAZODPeoKo+X1X/Pdr9InBkUlGDRhqApNkGLCVZH9ue2FHqMHBzbH9zdOxefgb480n9czFY6rkw1X00t6vq5EzOm/w0cBJ436S2Bo00ADO8M/gWcHRs/8jo2BvPlzwM/Arwvqp6bVJRp07SAEwxdZpkDTie5FiSQ8BZYOWN58p7gM8Ap6vqy02KGjTSAMzq8nZVbQHngCvAi8BzVXU1ycUkp0fNfhv4duCPk7yQZOUe5V7n1Enqu2lukmmgqlaB1R3HLoz9/fC0NQ0aaQB8eltSq3w5uaSDYdBIaptTJ0mt6/jD2waNNAQdzxmDRhqEjieNQSP1XB9efGXQSAPQ7ZgxaKRh6HjSGDRS7zV+qdXcGDTSAHR8icagkfpuyhdfzYVBIw2AUydJrXNEI6l1Hc8Zg0bqveav6Zwbg0YahG4njUEj9ZwvvpJ0IJw6SWqdl7clta/bOWPQSEPQ8ZwxaKS+m+IrlHPjlyqlAUjSaGtYaznJ9SQbSc7v8vtDSf4hyVaSH29S06CRBmBWn8RNsghcAh4FTgCPJTmxo9m/A48Dzzbtn1MnaQBmOHU6BWxU1Y3turkMnAGu3W1QVf82+u1/mhZ1RCP1Xhr/BywlWR/bnthR7DBwc2x/c3RsXxzRSD035ftoblfVyfZ6szuDRhqAGU6dbgFHx/aPjI7ti1MnaQCmmDpNsgYcT3IsySHgLLCy3/4ZNFLf5f/upZm0TVJVW8A54ArwIvBcVV1NcjHJaYAk702yCfwE8JkkVyfVdeok9VzTS9dNVdUqsLrj2IWxv9fYnlI1ZtBIQ9DxO4MNGmkAfHpbUut88ZWk9hk0ktrm1ElSq/rwpcpU1bz7IGkfkvwFsNSw+e2qWm6zP7sxaCS1zjuDJbXOoJHUOoNGUusMGkmtM2gkte5/Aa/EdHTSZv39AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(attention, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix resulting from the softmax is referred to as the attention matrix and is how much each matrix should pay attention to the others when we multiply our attention matrix by our matrix $E$ (corresponding to $V$). Try it out:\n",
    "\n",
    "- 5.1) This is essentially a weighted sum, one way to see this is to extract the weight from the first row of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0168,  0.4553, -0.2684, -0.4422,  1.4078,  0.6498, -0.4594,  0.3144,\n",
       "        -0.0282, -0.1918, -0.2583, -0.1760, -0.2305,  0.4504,  0.5662, -0.2919,\n",
       "        -0.2262,  0.4274, -0.3106,  0.0906, -0.4081,  1.2343, -0.0629,  0.3290,\n",
       "         0.3516, -2.4893, -0.6507,  0.3532,  0.1980, -0.2516,  2.4110, -0.4003,\n",
       "        -0.2784,  0.0607,  0.2499,  0.1769,  0.0999, -0.0049,  0.2741, -0.6425,\n",
       "        -0.1757,  0.4778,  0.0475,  0.1765,  0.5002, -0.4469, -0.4376, -1.0416,\n",
       "         0.4228, -0.0625])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = attention[0] # first row\n",
    "weighted = attn[0] * E[0] + attn[1] * E[1] + attn[2] * E[2]\n",
    "weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) now I want to create a function called attention to compute the scaled dot product attention presented above. It should return both the output, but also the attention matrix. Use to compute the scaled dot product attention of the embeddings `[\"man\", \"woman\", \"apple\", \"banana\", \"pineapple\"]` (you shouldn't use any learned weights).\n",
    "\n",
    "- 6.1) Examine the attention matrix, which words have a higher attention weight and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q,K,v):\n",
    "    d = v.shape[0]\n",
    "    QK = np.matmul(Q,K)/np.sqrt(d)\n",
    "\n",
    "    soft = torch.nn.Softmax(dim=0)\n",
    "    attn_mat = soft(torch.Tensor(QK))\n",
    "    return attn_mat"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94f20621c34d06f296d998be094ddbe636a1cbce433580f5c6b944e0692a01ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('nlp': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
